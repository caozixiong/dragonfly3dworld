<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dragonfly Daily 16: Denoising with Deep Learning in Dragonfly (2020) Summary</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
        }
        .timestamp {
            font-weight: bold;
            color: #007BFF;
        }
        .section-title {
            font-size: 1.2em;
            font-weight: bold;
            margin-top: 15px;
        }
        .subsection-title {
            font-weight: bold;
            margin-top: 10px;
        }
        ul {
            margin-top: 5px;
            padding-left: 20px;
        }
        li {
            margin-bottom: 3px;
        }
    </style>
</head>
<body>

    <h1>Dragonfly Daily 16: Denoising with Deep Learning in Dragonfly (2020) Summary</h1>

    <p>This video, presented by Mike Marsh, Product Manager of Dragonfly, discusses denoising techniques using deep learning within the Dragonfly 4.1 software. The lesson covers constructing neural network models for denoising, different training patterns, and practical demonstrations within the software.</p>

    <div class="section-title">Introduction [00:00:00]</div>
    <ul>
        <li>The video is part of a series (lessons 15-19) focusing on deep learning for imaging scientists.</li>
        <li>Lesson 16 specifically addresses denoising using deep learning methods.</li>
        <li>Future lessons will cover image segmentation, super-resolution, and advanced topics.</li>
        <li>Dragonfly 4.1 is used, requiring a CUDA-capable NVIDIA graphics card for deep learning functionalities.</li>
        <li>Hardware used: Mike's laptop (Windows 10, 16GB RAM, 6GB video memory) and a workstation (Windows 10, 32-core CPU, 64GB RAM, high-performance GeForce card).</li>
    </ul>

    <div class="section-title">Denoising with Deep Learning [00:02:55]</div>
    <ul>
        <li>The core concept involves training a neural network to transform noisy images into low-noise images.</li>
        <li>The training pattern focuses on pairing inputs (noisy images) with desired outputs (low-noise images).</li>
        <li>Three approaches to setting up a neural network model for denoising are discussed:
            <ul>
                <li>Pairing a high-noise image with a low-noise image [00:03:22].</li>
                <li>Using an autoencoder: pairing a high-noise image with a high-noise image [00:04:31].</li>
                <li>Noise-to-noise: training the model to convert a noisy image into another noisy image [00:04:49].</li>
            </ul>
        </li>
        <li>Low-noise images can be obtained empirically (longer exposure time) or synthetically (applying denoising filters) [00:05:23].</li>
        <li>Trained models are transferable and can be shared or uploaded to the Infinite Toolbox [00:06:16].</li>
    </ul>

    <div class="section-title">Demonstration in Dragonfly 4.1 [00:06:49]</div>
    <ul>
        <li>A multi-slice data set of HeLa cells acquired on a Zeiss Gemini 7 is used [00:06:55].</li>
        <li>The data set is noisy, motivating the need for denoising.</li>
        <li>A low-noise version of the image, pre-processed with image filters, is also loaded [00:08:13].</li>
        <li>The video demonstrates how to view noisy and denoised images side-by-side using synchronized scenes [00:08:46].</li>
        <li>Specific slices are extracted from both the noisy and denoised datasets to create training pairs [00:10:11].</li>
    </ul>

    <div class="section-title">Deep Learning Tool and Training [00:13:03]</div>
    <ul>
        <li>The Deep Learning Tool in Dragonfly is used to create and train denoising models.</li>
         <li>A new model is created using the U-Net architecture for regression (denoising) [00:14:15].</li>
        <li>The training process involves feeding the model paired noisy and denoised image slices [00:15:53].</li>
        <li>Training parameters, such as patch size, are adjusted [00:17:08].</li>
        <li>The training process is explained, including epochs, loss function, and validation data [00:17:26].</li>
        <li>The results of the trained model are previewed on a slice outside the training data [00:20:13].</li>
    </ul>

    <div class="section-title">Autoencoder Demonstration [00:21:05]</div>
    <ul>
        <li>An autoencoder model is created and trained using a high-noise image as both input and output.</li>
        <li>Despite the seemingly contradictory setup, the autoencoder effectively performs denoising due to its architecture [00:23:10].</li>
        <li>The results of the autoencoder are compared to the original noisy image [00:26:52].</li>
    </ul>

    <div class="section-title">Announcements and Q&A [00:27:43]</div>
    <ul>
        <li>Announcement of an interruption in the Dragonfly Daily schedule for April 30th and May 1st, encouraging viewers to register for an image processing techniques workshop [00:27:43].</li>
        <li>A Q&A session addresses various questions from the audience, including:
            <ul>
                <li>Filters used for creating the low noise image [00:28:59].</li>
                <li>Availability of literature on Dragonfly algorithms [00:29:37].</li>
                <li>Risks of over-interpreting data with denoising [00:30:12].</li>
                <li>Importance of geometric alignment in paired images [00:32:40].</li>
                <li>Comparison of validation and training error [00:33:43].</li>
                <li>Inputting a starting seed for the randomizer [00:35:39].</li>
                <li>Access to Python source code of deep learning models [00:36:12].</li>
                <li>Meaning of estimated memory ratio [00:36:43].</li>
                <li>Viewing the training history of the CNN [00:36:50].</li>
                <li>Advanced tutorial to edit the architecture of the model [00:38:21].</li>
                <li>Comparison of trained image to a filtered image [00:39:05].</li>
                <li>Use of data augmentation in the high-high example [00:42:30].</li>
                <li>Knowing which slices were picked for training [00:42:52].</li>
                <li>Comparison of deep learning denoising to a filter [00:43:35].</li>
                <li>Using Intel Open Image Denoise [00:43:45].</li>
                <li>Deep learning in a box [00:44:31].</li>
                <li>Tensorboard giving an empty webpage [00:44:51].</li>
                <li>Link to the SEM image event [00:45:03].</li>
                <li>Handling image artifacts like beam hardening [00:46:21].</li>
                <li>Numerical comparison of denoised image with ground truth [00:47:22].</li>
            </ul>
        </li>
    </ul>

    <p>The video concludes with a thank you to the audience and an invitation to join the next lesson on image segmentation.</p>

    <p><b>Metadata:</b></p>
    <ul>
        <li>Title: Dragonfly Daily 16 Denoising with Deep Learning in Dragonfly (2020)</li>
        <li>Channel name: Dragonfly</li>
        <li>Video length: PT49M30S</li>
        <li>View count: 6541</li>
        <li>Like count: 166</li>
        <li>Publish date: 2020-04-21</li>
    </ul>

</body>
</html>
